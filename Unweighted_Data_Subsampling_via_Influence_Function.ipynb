{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Unweighted Data Subsampling via Influence Function.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8JsMrkCrYw3"
      },
      "source": [
        "## **Basic imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIh_f_kK1G_g"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
        "import argparse\n",
        "import time\n",
        "import pdb\n",
        "import os\n",
        "np.random.seed(0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AdBdltarAyJ"
      },
      "source": [
        "# 1. **MNIST Dataset Load and Preprocessing the data in required format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV4aHBR41G_x"
      },
      "source": [
        "# select the dataset used\n",
        "dataset_name = \"mnist\"\n",
        "# regularization parameter for Logistic Regression\n",
        "C = 0.1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGuUulj3n4h",
        "outputId": "dd8d0905-9da0-40ab-ad1e-33d1352dbc8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# tool box\n",
        "def load_mnist(validation_size = 5000):\n",
        "    import gzip\n",
        "    def _read32(bytestream):\n",
        "        dt = np.dtype(np.uint32).newbyteorder(\">\")\n",
        "        return np.frombuffer(bytestream.read(4),dtype=dt)[0]\n",
        "\n",
        "    def extract_images(f):\n",
        "        print(\"Extracting\",f.name)\n",
        "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "            magic = _read32(bytestream)\n",
        "            num_images = _read32(bytestream)\n",
        "            rows = _read32(bytestream)\n",
        "            cols = _read32(bytestream)\n",
        "            buf = bytestream.read(rows * cols * num_images)\n",
        "            data = np.frombuffer(buf,dtype=np.uint8)\n",
        "            data = data.reshape(num_images,rows,cols,1)\n",
        "            return data\n",
        "    \n",
        "    def extract_labels(f):\n",
        "        print('Extracting', f.name)\n",
        "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "            magic = _read32(bytestream)\n",
        "            num_items = _read32(bytestream)\n",
        "            buf = bytestream.read(num_items)\n",
        "            labels = np.frombuffer(buf, dtype=np.uint8)\n",
        "            return labels\n",
        "\n",
        "    data_dir = \"./data\"\n",
        "    TRAIN_IMAGES = os.path.join(data_dir,'train-images-idx3-ubyte.gz')\n",
        "    with open(TRAIN_IMAGES,\"rb\") as f:\n",
        "        train_images = extract_images(f)\n",
        "\n",
        "    TRAIN_LABELS =  os.path.join(data_dir,'train-labels-idx1-ubyte.gz')\n",
        "    with open(TRAIN_LABELS,\"rb\") as f:\n",
        "        train_labels = extract_labels(f)\n",
        "\n",
        "    TEST_IMAGES =  os.path.join(data_dir,'t10k-images-idx3-ubyte.gz')\n",
        "    with open(TEST_IMAGES,\"rb\") as f:\n",
        "        test_images = extract_images(f)\n",
        "\n",
        "    TEST_LABELS =  os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    with open(TEST_LABELS,\"rb\") as f:\n",
        "        test_labels = extract_labels(f)\n",
        "\n",
        "    # split train and val\n",
        "    train_images = train_images[validation_size:]\n",
        "    train_labels = train_labels[validation_size:]\n",
        "\n",
        "    # preprocessing\n",
        "    train_images = train_images.astype(np.float32) / 255\n",
        "    test_images  = test_images.astype(np.float32) / 255\n",
        "    \n",
        "    # reshape for logistic regression\n",
        "    train_images = np.reshape(train_images, [train_images.shape[0], -1])\n",
        "    test_images = np.reshape(test_images, [test_images.shape[0], -1])\n",
        "    return train_images,train_labels,test_images,test_labels\n",
        "\n",
        "def load_data_two_class(dataset_name,va_ratio):\n",
        "  x_train,y_train,x_test,y_test = load_mnist()\n",
        "  pos_class = 1\n",
        "  neg_class = 7\n",
        "  x_train,y_train = filter_dataset(x_train,y_train,pos_class,neg_class)\n",
        "  x_va,y_va = filter_dataset(x_test,y_test,pos_class,neg_class)\n",
        "  y_va = y_va.astype(int)\n",
        "  y_train = y_train.astype(int)\n",
        "\n",
        "  num_va_sample = int((1-va_ratio) * x_train.shape[0])\n",
        "  x_val = x_train[num_va_sample:]\n",
        "  y_val = y_train[num_va_sample:]\n",
        "  x_train = x_train[:num_va_sample]\n",
        "  y_train = y_train[:num_va_sample]\n",
        "  x_te = x_va\n",
        "  y_te = y_va\n",
        "\n",
        "  return x_train,y_train,x_val,y_val,x_te,y_te\n",
        "\n",
        "\n",
        "def filter_dataset(X, Y, pos_class, neg_class, mode=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    Filters out elements of X and Y that aren't one of pos_class or neg_class\n",
        "    then transforms labels of Y so that +1 = pos_class, -1 = neg_class.\n",
        "\n",
        "    They are all 10-classes image classification data sets while Logistic regression can only handle binary classification. we\n",
        "    select the number 1 and 7 as positive and negative classes;\n",
        "    \"\"\"\n",
        "\n",
        "    assert(X.shape[0] == Y.shape[0])\n",
        "    assert(len(Y.shape) == 1)\n",
        "\n",
        "    Y = Y.astype(int)\n",
        "    \n",
        "    pos_idx = Y == pos_class\n",
        "    neg_idx = Y == neg_class        \n",
        "    Y[pos_idx] = 1\n",
        "    Y[neg_idx] = -1\n",
        "    idx_to_keep = pos_idx | neg_idx\n",
        "    X = X[idx_to_keep, ...]\n",
        "    Y = Y[idx_to_keep]\n",
        "    if Y.min() == -1 and mode != \"svm\":\n",
        "        Y = (Y + 1) / 2\n",
        "        Y.astype(int)\n",
        "    return (X, Y)\n",
        "\n",
        "# load data, pick 30% as the Va set\n",
        "x_train,y_train,x_va,y_va,x_te,y_te = load_data_two_class(dataset_name,va_ratio=0.3)\n",
        "\n",
        "\n",
        "\n",
        "print(\"x_train, nr sample {}, nr feature {}\".format(x_train.shape[0],x_train.shape[1]))\n",
        "print(\"x_va,    nr sample {}, nr feature {}\".format(x_va.shape[0],x_va.shape[1]))\n",
        "print(\"x_te,    nr sample {}, nr feature {}\".format(x_te.shape[0],x_te.shape[1]))\n",
        "print(\"Tr: Pos {} Neg {}\".format(y_train[y_train==1].shape[0],y_train[y_train==0].shape[0]))\n",
        "print(\"Va: Pos {} Neg {}\".format(y_va[y_va==1].shape[0],y_va[y_va==0].shape[0]))\n",
        "print(\"Te: Pos {} Neg {}\".format(y_te[y_te==1].shape[0],y_te[y_te==0].shape[0]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/train-images-idx3-ubyte.gz\n",
            "Extracting ./data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
            "x_train, nr sample 8325, nr feature 784\n",
            "x_va,    nr sample 3569, nr feature 784\n",
            "x_te,    nr sample 2163, nr feature 784\n",
            "Tr: Pos 4395 Neg 3930\n",
            "Va: Pos 1784 Neg 1785\n",
            "Te: Pos 1135 Neg 1028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIS_ygsKtHM2"
      },
      "source": [
        "# 2. **Flip labels for some of the images to make the data noisy so that we can show how our method discrad these kind of noisy data hence flipping the labels of some data in below code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ26meoM1HAF"
      },
      "source": [
        "# get the subset samples number\n",
        "num_tr_sample = x_train.shape[0]\n",
        "sample_ratio = 0.6\n",
        "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Our unweighted method can downweight the bad cases which cause high test loss to the our model, which is an important reason of its ability to improve result with less data.\n",
        "To show the performance of our methods in noisy label situation, we perform addtional experiments with some training\n",
        "labels being flipped. The results show the enlarging superiority of our subsampling methods \n",
        "\"\"\"\n",
        "\n",
        "# flip labels\n",
        "idxs = np.arange(y_train.shape[0])\n",
        "flip_ratio = 0.4\n",
        "np.random.shuffle(idxs)\n",
        "num_flip = int(flip_ratio * len(idxs))\n",
        "y_train[idxs[:num_flip]] = np.logical_xor(np.ones(num_flip), y_train[idxs[:num_flip]]).astype(int)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65j1_ru2txsq"
      },
      "source": [
        "# 3. **Training model on the full data set (ˆθ on the full Tr)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvhPW3cFtx89",
        "outputId": "f1d95a8b-d2b2-4e82-dc79-9cee9d8e41ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# define the full-set-model \n",
        "\n",
        "\"\"\" \n",
        "model : logistic regression:\n",
        "C: Inverse of regularization strength. smaller values specify stronger regularization.\n",
        "fit_intercept: constant (a.k.a. bias or intercept) should be added to the decision function.\n",
        "tol: Tolerance for stopping criteria.\n",
        "solver: Algorithm to use in the optimization problem.For small datasets, ‘liblinear’ is a good choice\n",
        "multi_class: ‘ovr’, then a binary problem is fit for each label.\n",
        "max_iterint: Maximum number of iterations taken for the solvers to converge\n",
        "\"\"\"\n",
        "\n",
        "clf = LogisticRegression(\n",
        "        C = C,\n",
        "        fit_intercept=False,\n",
        "        tol = 1e-8,\n",
        "        solver=\"liblinear\",\n",
        "        multi_class=\"ovr\",\n",
        "        max_iter=100,\n",
        "        warm_start=False,\n",
        "        verbose=1,\n",
        "        )\n",
        "\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "# on Va\n",
        "\n",
        "y_va_pred = clf.predict_proba(x_va)[:,1] \n",
        "#predict_proba : Probability estimates. The returned estimates for all classes are ordered by the label of classes.\n",
        "full_logloss = log_loss(y_va,y_va_pred) \n",
        "#Log loss, aka logistic loss or cross-entropy loss. This is the loss function defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. \n",
        "#For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is\n",
        "#-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
        "weight_ar = clf.coef_.flatten() \n",
        "#Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary.\n",
        "\n",
        "# on Te\n",
        "\n",
        "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
        "full_te_logloss = log_loss(y_te,y_te_pred)\n",
        "full_te_auc = roc_auc_score(y_te, y_te_pred)\n",
        "#The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
        "# The true-positive rate is also known as sensitivity, recall or probability of detection\n",
        "# The false-positive rate is also known as probability of false alarm \n",
        "# AUC measures how true positive rate (recall) and false positive rate trade off\n",
        "\n",
        "y_te_pred = clf.predict(x_te)\n",
        "full_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]\n",
        "\n",
        "\n",
        "# print full-set-model results\n",
        "print(\"[FullSet] Va logloss {:.6f}\".format(full_logloss))\n",
        "print(\"[FullSet] Te logloss {:.6f}\".format(full_te_logloss))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear][FullSet] Va logloss 0.525831\n",
            "[FullSet] Te logloss 0.522210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ5h8v3kTwrh"
      },
      "source": [
        "# 4. **compute the influence function (IF) for each sample in training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeP4tgowUNvS"
      },
      "source": [
        "def grad_logloss_theta_lr(label,ypred,x,C=0.03,has_l2=True,scale_factor=1.0):\n",
        "    \"\"\"Return d l_i / d_theta = d l_i / d_ypred * d y_pred / d theta\n",
        "        grad_logloss_theta: gradient on the theta, shape: [n,]\n",
        "    \"\"\"\n",
        "    # The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
        "    if not isinstance(label,np.ndarray) or not isinstance(ypred,np.ndarray):\n",
        "        label = np.array(label).flatten()\n",
        "        ypred = np.array(ypred).flatten()\n",
        "\n",
        "\n",
        "    grad_logloss_theta = C * x.T.dot(ypred-label)\n",
        "\n",
        "    return scale_factor * grad_logloss_theta\n",
        "\n",
        "def batch_grad_logloss_lr(label,ypred,x,C=0.03,scale_factor=1.0):\n",
        "    \"\"\"Return gradient on a batch.\n",
        "        batch_grad: gradient of each sample on parameters,\n",
        "            has shape [None,n]\n",
        "    \"\"\"\n",
        "    diffs = ypred - label\n",
        "    if isinstance(x,np.ndarray):\n",
        "        diffs = diffs.reshape(-1,1)\n",
        "        batch_grad = x * diffs\n",
        "    else:\n",
        "        diffs = sparse.diags(diffs)\n",
        "        batch_grad = x.T.dot(diffs).T\n",
        "    batch_grad = sparse.csr_matrix(C * batch_grad)      \n",
        "    return scale_factor * batch_grad"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhBwAgki1HAU"
      },
      "source": [
        "# building precoditioner\n",
        "test_grad_loss_val = grad_logloss_theta_lr(y_va,y_va_pred,x_va,C,0.1/(num_tr_sample*C))  #Return d l_i / d_theta\n",
        "\n",
        "tr_pred = clf.predict_proba(x_train)[:,1]\n",
        "batch_size = 10000\n",
        "M = None\n",
        "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
        "\n",
        "for idx in range(total_batch):\n",
        "    batch_tr_grad = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
        "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
        "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
        "        C,\n",
        "        1.0)\n",
        "\n",
        "    sum_grad = batch_tr_grad.multiply(x_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
        "    if M is None:\n",
        "        M = sum_grad\n",
        "    else:\n",
        "        M = M + sum_grad       \n",
        "M = M + 0.1/(num_tr_sample*C) * np.ones(x_train.shape[1])\n",
        "M = np.array(M).flatten()\n",
        "\n",
        "# computing the inverse Hessian-vector-product\n",
        "#The Hessian Matrix is a square matrix of second ordered partial derivatives of a scalar function.\n",
        "#It is of immense use in linear algebra as well as for determining points of local maxima or minima\n",
        "iv_hvp = inverse_hvp_lr_newtonCG(x_train,y_train,tr_pred,test_grad_loss_val,C,True,1e-5,True,M,0.1/(num_tr_sample*C))\n",
        "\n",
        "# get influence score\n",
        "total_batch = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
        "predicted_loss_diff = []\n",
        "for idx in range(total_batch):\n",
        "    train_grad_loss_val = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
        "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
        "        x_train[idx*batch_size:(idx+1)*batch_size],\n",
        "        C,\n",
        "        1.0)\n",
        "    predicted_loss_diff.extend(np.array(train_grad_loss_val.dot(iv_hvp)).flatten())    \n",
        "predicted_loss_diffs = np.asarray(predicted_loss_diff)\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"IF(inpulance function) Stats: mean {:.10f}, max {:.10f}, min {:.10f}\".format(\n",
        "    predicted_loss_diffs.mean(), predicted_loss_diffs.max(), predicted_loss_diffs.min())\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmwNiFGmlJG"
      },
      "source": [
        "# **5.compute the sampling probability of each sample in training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBSf9aEgm1mb"
      },
      "source": [
        "def select_from_one_class(y_train,prob_pi,label,ratio):\n",
        "    # select positive and negative samples respectively\n",
        "    num_sample = y_train[y_train==label].shape[0]\n",
        "    all_idx = np.arange(y_train.shape[0])[y_train==label]\n",
        "    label_prob_pi = prob_pi[all_idx]\n",
        "    obj_sample_size = int(ratio * num_sample)\n",
        "\n",
        "    sb_idx = None\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        rand_prob = np.random.rand(num_sample)\n",
        "        iter_idx = all_idx[rand_prob < label_prob_pi]\n",
        "        if sb_idx is None:\n",
        "            sb_idx = iter_idx\n",
        "        else:\n",
        "            new_idx = np.setdiff1d(iter_idx, sb_idx)\n",
        "            diff_size = obj_sample_size - sb_idx.shape[0]\n",
        "            if new_idx.shape[0] < diff_size:\n",
        "                sb_idx = np.union1d(iter_idx, sb_idx)\n",
        "            else:\n",
        "                new_idx = np.random.choice(new_idx, diff_size, replace=False)\n",
        "                sb_idx = np.union1d(sb_idx, new_idx)\n",
        "        iteration += 1\n",
        "        if sb_idx.shape[0] >= obj_sample_size:\n",
        "            sb_idx = np.random.choice(sb_idx,obj_sample_size,replace=False)\n",
        "            return sb_idx\n",
        "\n",
        "        if iteration > 100:\n",
        "            diff_size = obj_sample_size - sb_idx.shape[0]\n",
        "            leave_idx = np.setdiff1d(all_idx, sb_idx)\n",
        "            # left samples are sorted by their IF\n",
        "            # leave_idx = leave_idx[np.argsort(prob_pi[leave_idx])[-diff_size:]]\n",
        "            leave_idx = np.random.choice(leave_idx,diff_size,replace=False)\n",
        "            sb_idx = np.union1d(sb_idx, leave_idx)\n",
        "            return sb_idx"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZND6k2RY1HAf",
        "outputId": "904bf4b3-964b-4903-99fb-92a506dc0beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build sampling probability\n",
        "\n",
        "sigmoid_k = 10  # parameter for the sigmoid sampling function\n",
        "phi_ar = - predicted_loss_diffs\n",
        "IF_interval = phi_ar.max() - phi_ar.min()\n",
        "a_param = sigmoid_k / IF_interval\n",
        "prob_pi = 1 / (1 + np.exp(a_param * phi_ar))\n",
        "print(\"Pi Stats:\",np.percentile(prob_pi,[10,25,50,75,90]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pi Stats: [0.08010065 0.1627333  0.66298381 0.7917639  0.8593959 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJQYsVTynHWT"
      },
      "source": [
        "# **6. Do subsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qThZ8V7snHvD"
      },
      "source": [
        "pos_idx = select_from_one_class(y_train,prob_pi,1,sample_ratio)\n",
        "neg_idx = select_from_one_class(y_train,prob_pi,0,sample_ratio)\n",
        "sb_idx = np.union1d(pos_idx,neg_idx)\n",
        "sb_x_train = x_train[sb_idx]\n",
        "sb_y_train = y_train[sb_idx]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgx4f4lnVXo"
      },
      "source": [
        "# **7. train a subset-model on the reduced data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xTdpYUwnVgo",
        "outputId": "ec68939a-facb-4f8c-d323-e170900c5361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf.fit(sb_x_train,sb_y_train)\n",
        "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
        "sb_logloss = log_loss(y_va, y_va_pred)\n",
        "sb_weight = clf.coef_.flatten()\n",
        "diff_w_norm = np.linalg.norm(weight_ar - sb_weight)\n",
        "sb_size = sb_x_train.shape[0]\n",
        "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
        "sb_te_logloss = log_loss(y_te,y_te_pred)\n",
        "sb_te_auc = roc_auc_score(y_te, y_te_pred)\n",
        "y_te_pred = clf.predict(x_te)\n",
        "sb_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZgVvIwhniJ2"
      },
      "source": [
        "## **For comparison doing general method of subsampling i.e random subsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns02r4Ldnhjm",
        "outputId": "37961b9e-9d29-4813-9fe6-479c0d9a384b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# baseline model: random sampling\n",
        "\n",
        "u_idxs = np.arange(x_train.shape[0])\n",
        "uniform_idxs = np.random.choice(u_idxs,obj_sample_size,replace=False)\n",
        "us_x_train = x_train[uniform_idxs]\n",
        "us_y_train = y_train[uniform_idxs]\n",
        "clf.fit(us_x_train, us_y_train)\n",
        "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
        "us_logloss = log_loss(y_va, y_va_pred)\n",
        "us_size = us_x_train.shape[0]\n",
        "y_te_pred = clf.predict_proba(x_te)[:,1]\n",
        "us_te_logloss = log_loss(y_te,y_te_pred)\n",
        "us_te_auc = roc_auc_score(y_te, y_te_pred)\n",
        "y_te_pred = clf.predict(x_te)\n",
        "us_te_acc = (y_te == y_te_pred).sum() / y_te.shape[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTr5jkwnny2X"
      },
      "source": [
        "## **Result for comparision**\n",
        "## can be seen that our sigmoid UIDS log loss is much less as compared to random subsampling and full datset trained log loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dipLlzO7ntdY",
        "outputId": "139c49fc-6131-48c4-90c7-6894f04ee991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"==\"*30)\n",
        "print(\"Result Summary on Va\")\n",
        "print(\"[UIDS]  logloss {:.6f}, # {}\".format(sb_logloss,sb_size))\n",
        "print(\"[Random]   logloss {:.6f}, # {}\".format(us_logloss,us_size))\n",
        "print(\"[Full]     logloss {:.6f}, # {}\".format(full_logloss,num_tr_sample))\n",
        "\n",
        "print(\"Result Summary on Te\")\n",
        "print(\"[UIDS]  logloss {:.6f}, # {}\".format(sb_te_logloss,sb_size))\n",
        "print(\"[Random]   logloss {:.6f}, # {}\".format(us_te_logloss,us_size))\n",
        "print(\"[Full]     logloss {:.6f}, # {}\".format(full_te_logloss,num_tr_sample))\n",
        "print(\"==\"*30)\n",
        "# Attention: if the dataset used here is small, one experiment may fail because of uncertainty of subsampling!"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Result Summary on Va\n",
            "[UIDS]  logloss 0.162497, # 4994\n",
            "[Random]   logloss 0.527064, # 4995\n",
            "[Full]     logloss 0.525831, # 8325\n",
            "Result Summary on Te\n",
            "[UIDS]  logloss 0.176543, # 4994\n",
            "[Random]   logloss 0.524031, # 4995\n",
            "[Full]     logloss 0.522210, # 8325\n",
            "============================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6XxWsMo3xZ"
      },
      "source": [
        "## **Comparision of test accuracy on our UIDS technique , random subsamping and full dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwEqkYG01HAl",
        "outputId": "7fe0c145-150a-4392-c392-9f98efa914b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"==\"*30)\n",
        "print(\"Result Summary on Te (ACC and AUC)\")\n",
        "print(\"[UIDS]  acc {:.6f}, auc {:.6f} # {}\".format(sb_te_acc,sb_te_auc, sb_size))\n",
        "print(\"[Random]   acc {:.6f}, auc {:.6f} # {}\".format(us_te_acc,us_te_auc, us_size))\n",
        "print(\"[Full]     acc {:.6f}, auc {:.6f} # {}\".format(full_te_acc,full_te_auc, num_tr_sample))\n",
        "print(\"==\"*30)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Result Summary on Te (ACC and AUC)\n",
            "[UIDS]  acc 0.984281, auc 0.998802 # 4994\n",
            "[Random]   acc 0.900139, auc 0.950359 # 4995\n",
            "[Full]     acc 0.916782, auc 0.961824 # 8325\n",
            "============================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}